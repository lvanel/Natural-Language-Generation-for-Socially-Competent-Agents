{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from statistics import mean\n",
    "import pandas as pd\n",
    "import math\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Match them back to their models\n",
    "ref = \"data/full_responses_french_v3_annots.csv\"\n",
    "data_ref = pd.read_csv(ref, encoding = 'UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Output of STEPS 1 & 2: Filter and Top-3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_batch(batch):\n",
    "    for col in batch.columns:\n",
    "        if col == 'context':\n",
    "            batch[col] = list(batch[col].values)\n",
    "\n",
    "        else:\n",
    "            batch[col] = batch[col].apply(lambda x: literal_eval(x) if \"[\" in x else x)\n",
    "    \n",
    "    return batch.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators = ['Audrey', 'Cindy', 'Tiphaine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>annotators</th>\n",
       "      <th>eliminated</th>\n",
       "      <th>filtered</th>\n",
       "      <th>top-3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Customer(C): Euh ouais bah euh ouais demain di...</td>\n",
       "      <td>[Audrey, Cindy]</td>\n",
       "      <td>[[Oui, je vais vérifier si [Ville d'un centre]...</td>\n",
       "      <td>[[D'accord, je vais vérifier ça pour vous. Si ...</td>\n",
       "      <td>[[Super, je vais donc organiser une installati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Customer(C): D'accord et comment ça se passe i...</td>\n",
       "      <td>[Audrey, Tiphaine]</td>\n",
       "      <td>[[Bien sûr, nous pouvons organiser une visite ...</td>\n",
       "      <td>[[Le centre de réparation automobile est situé...</td>\n",
       "      <td>[[Je peux vous donner une estimation des frais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Customer(C): Alors attendez une seconde une se...</td>\n",
       "      <td>[Audrey, Tiphaine]</td>\n",
       "      <td>[[D'accord, je suis là. Pouvez-vous me dire ce...</td>\n",
       "      <td>[[Quel est le modèle de votre véhicule, madame...</td>\n",
       "      <td>[[Avez-vous trouvé la carte verte, madame?, D'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Customer(C): Ouais f r ? vous voulez le f r ? ...</td>\n",
       "      <td>[Audrey, Tiphaine]</td>\n",
       "      <td>[[Merci pour les informations fournies. Votre ...</td>\n",
       "      <td>[[Merci, j'ai tout noté. Pouvez-vous me donner...</td>\n",
       "      <td>[[Merci, j'ai tout noté. Pouvez-vous me donner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Customer(C): J'ai pas d'autre chose j'ai pas d...</td>\n",
       "      <td>[Audrey, Tiphaine]</td>\n",
       "      <td>[[\"Je vais essayer de vous mettre en communica...</td>\n",
       "      <td>[[D'accord, je vais vous mettre en communicati...</td>\n",
       "      <td>[[D'accord, je vais vous mettre en communicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Customer(C): Oui d'accord j'ai pas entendu voi...</td>\n",
       "      <td>[Audrey, Tiphaine]</td>\n",
       "      <td>[[D'accord, votre adresse est bien 12 rue Marg...</td>\n",
       "      <td>[[D'accord, merci pour les informations. Pourr...</td>\n",
       "      <td>[[\"Okay, madame, je vais mettre à jour votre a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Customer(C): Ouais non [Nom de ville (Client)]...</td>\n",
       "      <td>[Audrey, Tiphaine]</td>\n",
       "      <td>[[D'accord, je vais essayer de contacter le ce...</td>\n",
       "      <td>[[Oui, je comprends parfaitement votre situati...</td>\n",
       "      <td>[[\"D'accord, je vais appeler le centre pour vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Customer(C): Bah ah ouais ? et euh sur [Ville ...</td>\n",
       "      <td>[Audrey, Tiphaine]</td>\n",
       "      <td>[[Bonjour, je vérifie pour vous. Oui, nous avo...</td>\n",
       "      <td>[[D'accord, je vais vérifier pour vous. Pouvez...</td>\n",
       "      <td>[[D'accord, je vais vérifier pour vous. Pouvez...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Customer(C): Oui il vous faut quoi ? oui oui l...</td>\n",
       "      <td>[Audrey, Tiphaine]</td>\n",
       "      <td>[[Madame, je suis désolé, mais il semble y avo...</td>\n",
       "      <td>[[Oui madame, je comprends. Pouvez-vous me dir...</td>\n",
       "      <td>[[Non, madame, je veux savoir si votre carte e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Customer(C): Oui bonjour madame je vous appell...</td>\n",
       "      <td>[Audrey, Tiphaine]</td>\n",
       "      <td>[[D'accord, je vais vous laisser un peu de tem...</td>\n",
       "      <td>[[D'accord, prenez votre temps, je vais attend...</td>\n",
       "      <td>[[D'accord, prenez votre temps, je vais attend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context          annotators  \\\n",
       "0  Customer(C): Euh ouais bah euh ouais demain di...     [Audrey, Cindy]   \n",
       "1  Customer(C): D'accord et comment ça se passe i...  [Audrey, Tiphaine]   \n",
       "2  Customer(C): Alors attendez une seconde une se...  [Audrey, Tiphaine]   \n",
       "3  Customer(C): Ouais f r ? vous voulez le f r ? ...  [Audrey, Tiphaine]   \n",
       "4  Customer(C): J'ai pas d'autre chose j'ai pas d...  [Audrey, Tiphaine]   \n",
       "5  Customer(C): Oui d'accord j'ai pas entendu voi...  [Audrey, Tiphaine]   \n",
       "6  Customer(C): Ouais non [Nom de ville (Client)]...  [Audrey, Tiphaine]   \n",
       "7  Customer(C): Bah ah ouais ? et euh sur [Ville ...  [Audrey, Tiphaine]   \n",
       "8  Customer(C): Oui il vous faut quoi ? oui oui l...  [Audrey, Tiphaine]   \n",
       "9  Customer(C): Oui bonjour madame je vous appell...  [Audrey, Tiphaine]   \n",
       "\n",
       "                                          eliminated  \\\n",
       "0  [[Oui, je vais vérifier si [Ville d'un centre]...   \n",
       "1  [[Bien sûr, nous pouvons organiser une visite ...   \n",
       "2  [[D'accord, je suis là. Pouvez-vous me dire ce...   \n",
       "3  [[Merci pour les informations fournies. Votre ...   \n",
       "4  [[\"Je vais essayer de vous mettre en communica...   \n",
       "5  [[D'accord, votre adresse est bien 12 rue Marg...   \n",
       "6  [[D'accord, je vais essayer de contacter le ce...   \n",
       "7  [[Bonjour, je vérifie pour vous. Oui, nous avo...   \n",
       "8  [[Madame, je suis désolé, mais il semble y avo...   \n",
       "9  [[D'accord, je vais vous laisser un peu de tem...   \n",
       "\n",
       "                                            filtered  \\\n",
       "0  [[D'accord, je vais vérifier ça pour vous. Si ...   \n",
       "1  [[Le centre de réparation automobile est situé...   \n",
       "2  [[Quel est le modèle de votre véhicule, madame...   \n",
       "3  [[Merci, j'ai tout noté. Pouvez-vous me donner...   \n",
       "4  [[D'accord, je vais vous mettre en communicati...   \n",
       "5  [[D'accord, merci pour les informations. Pourr...   \n",
       "6  [[Oui, je comprends parfaitement votre situati...   \n",
       "7  [[D'accord, je vais vérifier pour vous. Pouvez...   \n",
       "8  [[Oui madame, je comprends. Pouvez-vous me dir...   \n",
       "9  [[D'accord, prenez votre temps, je vais attend...   \n",
       "\n",
       "                                               top-3  \n",
       "0  [[Super, je vais donc organiser une installati...  \n",
       "1  [[Je peux vous donner une estimation des frais...  \n",
       "2  [[Avez-vous trouvé la carte verte, madame?, D'...  \n",
       "3  [[Merci, j'ai tout noté. Pouvez-vous me donner...  \n",
       "4  [[D'accord, je vais vous mettre en communicati...  \n",
       "5  [[\"Okay, madame, je vais mettre à jour votre a...  \n",
       "6  [[\"D'accord, je vais appeler le centre pour vo...  \n",
       "7  [[D'accord, je vais vérifier pour vous. Pouvez...  \n",
       "8  [[Non, madame, je veux savoir si votre carte e...  \n",
       "9  [[D'accord, prenez votre temps, je vais attend...  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/final_results_filter_DEV_NEW_DPO_clean.csv\", encoding='UTF-8')\n",
    "df = prep_batch(df)\n",
    "\n",
    "pd.DataFrame(df).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = df['context']\n",
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lists(list_of_lists):\n",
    "    unique = []\n",
    "\n",
    "    for list in list_of_lists:\n",
    "        for elem in list:\n",
    "            if elem not in unique:\n",
    "                unique.append(elem)\n",
    "\n",
    "    return unique\n",
    "\n",
    "\n",
    "def merge_tops(list_of_lists):\n",
    "    final_top = []\n",
    "    if len(list_of_lists)==2:\n",
    "        final_top = [x for x in list_of_lists[0] if x in list_of_lists[1]]\n",
    "    \n",
    "    #elif len(list_of_lists)==3:\n",
    "    #    final_top = [x for x in list_of_lists[0] if x in list_of_lists[1] and x in list_of_lists[2]]\n",
    "    \n",
    "    else:\n",
    "        print('OOPS', len(list_of_lists))\n",
    "    \n",
    "    return final_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_responses_to_model(context, responses, model_responses):\n",
    "    model_responses_contexts = model_responses['context'].tolist()\n",
    "    index_context = model_responses_contexts.index(context)\n",
    "\n",
    "    cols = [col for col in model_responses.columns if col not in ['context']]\n",
    "    row = model_responses.iloc[index_context]\n",
    "    #print(row)\n",
    "\n",
    "    models=[]\n",
    "    for response in responses:\n",
    "        response = response\n",
    "        for col in cols:\n",
    "            #print(\"COL: \", col)\n",
    "            if isinstance(row[col], str):\n",
    "                if response == row[col].strip():\n",
    "                    models.append(col)\n",
    "    \n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qwen naive planning', 'qwen response e2e', 'qwen 2 step planning', 'qwen 3 steps planning', 'llama naive planning', 'llama response e2e', 'llama 2 step planning', 'llama 3 steps planning', 'mixtral naive planning', 'mixtral response e2e', 'mixtral 2 step planning', 'mixtral 3 steps planning', 'qwen dpo response e2e', 'qwen dpo naive planning']\n"
     ]
    }
   ],
   "source": [
    "models = [x for x in data_ref.columns if x not in [\"context\", 'emotions', 'strats', \"gt segments\", \"gt segment strats\", 'ground truth']]\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_per_annot_models = {'context': contexts, 'annotators': df['annotators'], 'eliminated': [], 'filtered': [], 'top-3': []}\n",
    "for i in range(len(contexts)):\n",
    "    ctx = contexts[i]\n",
    "    dic_per_annot_models['eliminated'].append([match_responses_to_model(ctx, df['eliminated'][i][0], data_ref), match_responses_to_model(ctx, df['eliminated'][i][1], data_ref)])\n",
    "    dic_per_annot_models['filtered'].append([match_responses_to_model(ctx, df['filtered'][i][0], data_ref), match_responses_to_model(ctx, df['filtered'][i][1], data_ref)])\n",
    "    dic_per_annot_models['top-3'].append([match_responses_to_model(ctx, df['top-3'][i][0], data_ref), match_responses_to_model(ctx, df['top-3'][i][1], data_ref)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_models_filter = {annot:{model: 0 for model in models} for annot in annotators}\n",
    "stats_models_top3 = {annot:{model: 0 for model in models} for annot in annotators}\n",
    "stats_models_eliminated = {annot:{model: 0 for model in models} for annot in annotators}\n",
    "\n",
    "\n",
    "for i in range(len(contexts)):\n",
    "    annots = dic_per_annot_models['annotators'][i]\n",
    "    for j, annotator in enumerate(annots):\n",
    "\n",
    "        filtered_models = dic_per_annot_models['filtered'][i][j]\n",
    "        for model in filtered_models:\n",
    "            stats_models_filter[annotator][model] += 1\n",
    "\n",
    "        eliminated_models = dic_per_annot_models['eliminated'][i][j]\n",
    "        for model in eliminated_models:\n",
    "            stats_models_eliminated[annotator][model] += 1\n",
    "\n",
    "        top_3_models = dic_per_annot_models['top-3'][i][j]\n",
    "        for model in top_3_models:\n",
    "            stats_models_top3[annotator][model] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_m = {m:{2:0, 1:0, 0:0} for m in models}\n",
    "top_m = {m:{2:0, 1:0, 0:0} for m in models}\n",
    "\n",
    "for i, ctx in enumerate(contexts):\n",
    "    for model in models:\n",
    "        if model in dic_per_annot_models['filtered'][i][0] and model in dic_per_annot_models['filtered'][i][1]:\n",
    "            filter_m[model][2] +=1\n",
    "\n",
    "        \n",
    "        elif model in dic_per_annot_models['filtered'][i][0] or model in dic_per_annot_models['filtered'][i][1]:\n",
    "            filter_m[model][1] +=1\n",
    "        \n",
    "        else:\n",
    "            filter_m[model][0] +=1\n",
    "\n",
    "\n",
    "        if model in dic_per_annot_models['top-3'][i][0] and model in dic_per_annot_models['top-3'][i][1]:\n",
    "            top_m[model][2] +=1\n",
    "\n",
    "        elif model in dic_per_annot_models['top-3'][i][0] or model in dic_per_annot_models['top-3'][i][1]:\n",
    "            top_m[model][1] +=1\n",
    "        \n",
    "        else:\n",
    "            top_m[model][0] +=1\n",
    "\n",
    "scores_models_filter = {m : (v[2] + v[1]) for m, v in zip(models, filter_m.values())}\n",
    "scores_models_top3 = {m : (v[2] + v[1]) for m, v in zip(models, top_m.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {m:{'filtered': 0, 'filtered weighted':0, 'top-3': 0, 'top@1':0, 'top@2':0, 'top@3':0} for m in models}\n",
    "for model in (scores_models_filter.keys()):\n",
    "    v = filter_m[model]\n",
    "    scores[model]['filtered'] = (v[2] + v[1]) / len(contexts)\n",
    "    scores[model]['filtered weighted'] = round((1.5*v[2] + v[1]) / len(contexts),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen naive planning 0.65 0.81\n",
      "qwen response e2e 0.55 0.69\n",
      "qwen 2 step planning 0.23 0.27\n",
      "qwen 3 steps planning 0.23 0.28\n",
      "llama naive planning 0.7 0.92\n",
      "llama response e2e 0.67 0.81\n",
      "llama 2 step planning 0.42 0.52\n",
      "llama 3 steps planning 0.51 0.67\n",
      "mixtral naive planning 0.63 0.85\n",
      "mixtral response e2e 0.72 0.92\n",
      "mixtral 2 step planning 0.29 0.34\n",
      "mixtral 3 steps planning 0.25 0.3\n",
      "qwen dpo response e2e 0.03 0.04\n",
      "qwen dpo naive planning 0.04 0.05\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model, scores[model]['filtered'], scores[model]['filtered weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST FILTERED MODELS\n",
      "\n",
      "mixtral response e2e \t\t 72 \t-   SELECTED TWICE:  40 , SELECTED ONCE:  32 , NOT SELECTED:  28\n",
      "llama naive planning \t\t 70 \t-   SELECTED TWICE:  43 , SELECTED ONCE:  27 , NOT SELECTED:  30\n",
      "llama response e2e \t\t 67 \t-   SELECTED TWICE:  27 , SELECTED ONCE:  40 , NOT SELECTED:  33\n",
      "qwen naive planning \t\t 65 \t-   SELECTED TWICE:  33 , SELECTED ONCE:  32 , NOT SELECTED:  35\n",
      "mixtral naive planning \t\t 63 \t-   SELECTED TWICE:  45 , SELECTED ONCE:  18 , NOT SELECTED:  37\n",
      "qwen response e2e \t\t 55 \t-   SELECTED TWICE:  27 , SELECTED ONCE:  28 , NOT SELECTED:  45\n",
      "llama 3 steps planning \t\t 51 \t-   SELECTED TWICE:  31 , SELECTED ONCE:  20 , NOT SELECTED:  49\n",
      "llama 2 step planning \t\t 42 \t-   SELECTED TWICE:  19 , SELECTED ONCE:  23 , NOT SELECTED:  58\n",
      "mixtral 2 step planning \t\t 29 \t-   SELECTED TWICE:  9 , SELECTED ONCE:  20 , NOT SELECTED:  71\n",
      "mixtral 3 steps planning \t\t 25 \t-   SELECTED TWICE:  11 , SELECTED ONCE:  14 , NOT SELECTED:  75\n",
      "qwen 2 step planning \t\t 23 \t-   SELECTED TWICE:  8 , SELECTED ONCE:  15 , NOT SELECTED:  77\n",
      "qwen 3 steps planning \t\t 23 \t-   SELECTED TWICE:  9 , SELECTED ONCE:  14 , NOT SELECTED:  77\n",
      "qwen dpo naive planning \t\t 4 \t-   SELECTED TWICE:  2 , SELECTED ONCE:  2 , NOT SELECTED:  96\n",
      "qwen dpo response e2e \t\t 3 \t-   SELECTED TWICE:  1 , SELECTED ONCE:  2 , NOT SELECTED:  97\n"
     ]
    }
   ],
   "source": [
    "scores_models_filter = {k: v for k, v in sorted(scores_models_filter.items(), reverse= True, key=lambda item: item[1])}\n",
    "\n",
    "print(\"MOST FILTERED MODELS\\n\")\n",
    "for model in (scores_models_filter.keys()):\n",
    "    v = filter_m[model]\n",
    "    print(model, '\\t\\t', v[2] + v[1], \"\\t-   SELECTED TWICE: \", v[2], ', SELECTED ONCE: ', v[1], ', NOT SELECTED: ', v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST SELECTED MODELS AS TOP-3\n",
      "\n",
      "llama naive planning \t\t 58 \t-   SELECTED TWICE:  25 , SELECTED ONCE:  33 , NOT SELECTED:  42\n",
      "mixtral naive planning \t\t 53 \t-   SELECTED TWICE:  28 , SELECTED ONCE:  25 , NOT SELECTED:  47\n",
      "mixtral response e2e \t\t 48 \t-   SELECTED TWICE:  25 , SELECTED ONCE:  23 , NOT SELECTED:  52\n",
      "qwen naive planning \t\t 46 \t-   SELECTED TWICE:  18 , SELECTED ONCE:  28 , NOT SELECTED:  54\n",
      "llama response e2e \t\t 40 \t-   SELECTED TWICE:  20 , SELECTED ONCE:  20 , NOT SELECTED:  60\n",
      "qwen response e2e \t\t 35 \t-   SELECTED TWICE:  11 , SELECTED ONCE:  24 , NOT SELECTED:  65\n",
      "llama 3 steps planning \t\t 34 \t-   SELECTED TWICE:  11 , SELECTED ONCE:  23 , NOT SELECTED:  66\n",
      "llama 2 step planning \t\t 29 \t-   SELECTED TWICE:  8 , SELECTED ONCE:  21 , NOT SELECTED:  71\n",
      "mixtral 2 step planning \t\t 16 \t-   SELECTED TWICE:  4 , SELECTED ONCE:  12 , NOT SELECTED:  84\n",
      "mixtral 3 steps planning \t\t 10 \t-   SELECTED TWICE:  4 , SELECTED ONCE:  6 , NOT SELECTED:  90\n",
      "qwen 2 step planning \t\t 9 \t-   SELECTED TWICE:  1 , SELECTED ONCE:  8 , NOT SELECTED:  91\n",
      "qwen 3 steps planning \t\t 9 \t-   SELECTED TWICE:  2 , SELECTED ONCE:  7 , NOT SELECTED:  91\n",
      "qwen dpo response e2e \t\t 2 \t-   SELECTED TWICE:  0 , SELECTED ONCE:  2 , NOT SELECTED:  98\n",
      "qwen dpo naive planning \t\t 2 \t-   SELECTED TWICE:  0 , SELECTED ONCE:  2 , NOT SELECTED:  98\n"
     ]
    }
   ],
   "source": [
    "scores_models_top3 = {k: v for k, v in sorted(scores_models_top3.items(), reverse= True, key=lambda item: item[1])}\n",
    "\n",
    "print(\"MOST SELECTED MODELS AS TOP-3\\n\")\n",
    "for model in (scores_models_top3.keys()):\n",
    "    v = top_m[model]\n",
    "    print(model, '\\t\\t', v[2] +v[1], \"\\t-   SELECTED TWICE: \", v[2], ', SELECTED ONCE: ', v[1], ', NOT SELECTED: ', v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_ranked = {1:{m:0 for m in models}, 2:{m:0 for m in models}, 3:{m:0 for m in models}, 4:{m:0 for m in models}}\n",
    "\n",
    "for tops in dic_per_annot_models['top-3']:\n",
    "    for top in tops:\n",
    "        for i in range(len(top)):\n",
    "            top_3_ranked[i+1][top[i]] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models: \n",
    "    v = []\n",
    "    for i in range(1,4):\n",
    "        v.append(top_3_ranked[i][m])\n",
    "    scores[m]['top-3'] = round((v[0] + v[1] + v[2]) / len(contexts), 2)\n",
    "\n",
    "    if scores[m]['top-3'] > 0:\n",
    "        scores[m]['top@1'] = round(((v[0] / scores[m]['top-3'])/len(contexts))*scores[m]['top-3'],2) \n",
    "        scores[m]['top@2'] = round((((v[1]) / scores[m]['top-3'])/len(contexts))*scores[m]['top-3'],2)\n",
    "        scores[m]['top@3'] = round((((v[2]) / scores[m]['top-3'])/len(contexts))*scores[m]['top-3'],2)\n",
    "\n",
    "    else:\n",
    "        scores[m]['top-1'] = 0\n",
    "        scores[m]['top-2'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- SELECTED AS  1\n",
      "qwen naive planning 64\n",
      "llama naive planning 40\n",
      "qwen response e2e 32\n",
      "llama response e2e 24\n",
      "llama 2 step planning 13\n",
      "mixtral naive planning 10\n",
      "qwen 2 step planning 4\n",
      "mixtral response e2e 4\n",
      "qwen 3 steps planning 2\n",
      "llama 3 steps planning 2\n",
      "mixtral 3 steps planning 1\n",
      " ---- SELECTED AS  2\n",
      "llama naive planning 38\n",
      "mixtral naive planning 34\n",
      "llama response e2e 25\n",
      "llama 3 steps planning 25\n",
      "llama 2 step planning 15\n",
      "qwen response e2e 14\n",
      "mixtral response e2e 14\n",
      "qwen 2 step planning 6\n",
      "qwen 3 steps planning 6\n",
      "mixtral 2 step planning 4\n",
      "mixtral 3 steps planning 1\n",
      "qwen dpo response e2e 1\n",
      " ---- SELECTED AS  3\n",
      "mixtral response e2e 55\n",
      "mixtral naive planning 37\n",
      "llama 3 steps planning 18\n",
      "mixtral 2 step planning 15\n",
      "llama response e2e 10\n",
      "mixtral 3 steps planning 9\n",
      "llama 2 step planning 7\n",
      "llama naive planning 5\n",
      "qwen 3 steps planning 3\n",
      "qwen dpo naive planning 2\n",
      "qwen dpo response e2e 1\n",
      " ---- SELECTED AS  4\n",
      "mixtral 3 steps planning 3\n",
      "llama 2 step planning 2\n",
      "llama response e2e 1\n",
      "mixtral 2 step planning 1\n"
     ]
    }
   ],
   "source": [
    "for i in top_3_ranked.keys():\n",
    "    print(' ---- SELECTED AS ', i)\n",
    "    topi = {k: v for k, v in sorted(top_3_ranked[i].items(), key=lambda item: item[1], reverse=True)}\n",
    "    for k, v in topi.items():\n",
    "        if v >0:\n",
    "            print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mtx = np.zeros((len(models), len(scores[models[0]])))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for j, score in enumerate(list(scores[model].keys())):\n",
    "        mtx[i][j] = scores[model][score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          filtered  filtered weighted  top-3  top@1  top@2  \\\n",
      "qwen naive planning           0.65               0.81   0.64   0.64   0.00   \n",
      "qwen response e2e             0.55               0.69   0.46   0.32   0.14   \n",
      "qwen 2 step planning          0.23               0.27   0.10   0.04   0.06   \n",
      "qwen 3 steps planning         0.23               0.28   0.11   0.02   0.06   \n",
      "llama naive planning          0.70               0.92   0.83   0.40   0.38   \n",
      "llama response e2e            0.67               0.81   0.59   0.24   0.25   \n",
      "llama 2 step planning         0.42               0.52   0.35   0.13   0.15   \n",
      "llama 3 steps planning        0.51               0.67   0.45   0.02   0.25   \n",
      "mixtral naive planning        0.63               0.85   0.81   0.10   0.34   \n",
      "mixtral response e2e          0.72               0.92   0.73   0.04   0.14   \n",
      "mixtral 2 step planning       0.29               0.34   0.19   0.00   0.04   \n",
      "mixtral 3 steps planning      0.25               0.30   0.11   0.01   0.01   \n",
      "qwen dpo response e2e         0.03               0.04   0.02   0.00   0.01   \n",
      "qwen dpo naive planning       0.04               0.05   0.02   0.00   0.00   \n",
      "\n",
      "                          top@3  \n",
      "qwen naive planning        0.00  \n",
      "qwen response e2e          0.00  \n",
      "qwen 2 step planning       0.00  \n",
      "qwen 3 steps planning      0.03  \n",
      "llama naive planning       0.05  \n",
      "llama response e2e         0.10  \n",
      "llama 2 step planning      0.07  \n",
      "llama 3 steps planning     0.18  \n",
      "mixtral naive planning     0.37  \n",
      "mixtral response e2e       0.55  \n",
      "mixtral 2 step planning    0.15  \n",
      "mixtral 3 steps planning   0.09  \n",
      "qwen dpo response e2e      0.01  \n",
      "qwen dpo naive planning    0.02  \n"
     ]
    }
   ],
   "source": [
    "panda_df = pd.DataFrame(data = mtx,  \n",
    "                        index = models,  \n",
    "                        columns = list(scores[model].keys())\n",
    ") \n",
    "print(panda_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_scores = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n"
     ]
    }
   ],
   "source": [
    "#SAVE IN TXT FILE\n",
    "file_generated = \"filter_top_results\"\n",
    "print('Saving results...')\n",
    "with open(\"results/\"+file_generated+\".txt\", \"w\") as f:\n",
    "  print(panda_df, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Output of STEPS 3: Fine-Grained Socio-Emotional Annotation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Individual Scores</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators = ['Amira', 'Cindy', 'Hayoung', 'Tiphaine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/human_eval_socemo_annot.csv\"\n",
    "data = pd.read_csv(path, encoding='UTF-8')\n",
    "\n",
    "dic_of_ratings = {x:list(data[x]) for x in data.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['model', 'response', 'context']\n",
    "ratings = ['annotated_strategy',\"usefulness\",\"fluency\",\"emotion\",\"emotion_adequate\",\"strategy_adequate\",\"role_consistency\", 'global_consistency']\n",
    "\n",
    "all_cols = cols + ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qwen naive planning',\n",
       " 'qwen response e2e',\n",
       " 'llama naive planning',\n",
       " 'llama response e2e',\n",
       " 'mixtral naive planning',\n",
       " 'mixtral response e2e',\n",
       " 'ground truth']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['qwen naive planning','qwen response e2e','llama naive planning','llama response e2e','mixtral naive planning',\n",
    " 'mixtral response e2e','ground truth']\n",
    " \n",
    "models\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_model_criteria = {m:{h:[] for h in list(dic_of_ratings.keys()) if h != 'model'} for m in models}\n",
    "\n",
    "for i in range(len(dic_of_ratings['model'])):\n",
    "    m = dic_of_ratings['model'][i]\n",
    "    \n",
    "    for col in list(dic_model_criteria[m].keys()):\n",
    "        dic_model_criteria[m][col].append(dic_of_ratings[col][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'response', 'context', 'annotated_strategy', 'usefulness', 'fluency', 'emotion', 'emotion_adequate', 'strategy_adequate', 'role_consistency', 'naturalness', 'gt emotion', 'gt segment strats', 'gt segments', 'pred labels'])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_model_criteria[m].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textdistance import levenshtein\n",
    "import distance\n",
    "\n",
    "for m in models:\n",
    "    ref = [literal_eval(x) for x in dic_model_criteria[m]['gt segment strats']]\n",
    "    annot =  dic_model_criteria[m]['pred labels']\n",
    "    dic_model_criteria[m]['lev strat'] = round(mean([levenshtein.normalized_similarity(a,b) for a,b in zip(ref, annot)]),2)\n",
    "    dic_model_criteria[m]['jacc strat'] = round(mean([distance.jaccard(a,b) for a,b in zip(ref, annot)]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "\n",
    "for m in models:\n",
    "    ref = dic_model_criteria[m]['gt emotion']\n",
    "    annot =  dic_model_criteria[m]['emotion']\n",
    "    dic_model_criteria[m]['emo f1'] = round(f1_score(annot,ref, average='weighted'),2)\n",
    "    #dic_model_criteria[m]['emo f1'] = round(accuracy_score(annot,ref),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_model_criteria[m].keys()\n",
    "\n",
    "cols = ['usefulness', 'fluency', 'naturalness', 'emo f1', 'emotion_adequate', 'strategy_adequate', 'lev strat', 'jacc strat', 'role_consistency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "scores_all = {m:{} for m in models}\n",
    "mtx = np.zeros((len(models), len(cols)))\n",
    "\n",
    "for i, m in enumerate(models):\n",
    "    for j, col in enumerate(cols):\n",
    "        if isinstance(dic_model_criteria[m][col], list):\n",
    "            mtx[i][j] = round(mean(dic_model_criteria[m][col]),2)\n",
    "            scores_all[m][col] = round(mean(dic_model_criteria[m][col]),2)\n",
    "        \n",
    "        else:\n",
    "            mtx[i][j] = round(dic_model_criteria[m][col],2)\n",
    "            scores_all[m][col] = round(dic_model_criteria[m][col],2)\n",
    "\n",
    "\n",
    "        if col in ['usefulness',  'fluency',  'emotion_adequate', 'strategy_adequate', 'naturalness']:\n",
    "            mtx[i][j] = round(mtx[i][j] /3,2)\n",
    "            scores_all[m][col] = round(scores_all[m][col] /3,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        usefulness  fluency  naturalness  emo f1  \\\n",
      "qwen naive planning           0.92     0.92         0.78    0.92   \n",
      "qwen response e2e             0.70     0.88         0.72    0.73   \n",
      "llama naive planning          0.88     0.88         0.82    0.74   \n",
      "llama response e2e            0.87     0.92         0.78    0.95   \n",
      "mixtral naive planning        0.83     0.90         0.83    0.86   \n",
      "mixtral response e2e          0.85     0.90         0.70    0.90   \n",
      "ground truth                  0.87     0.92         0.88    0.68   \n",
      "\n",
      "                        emotion_adequate  strategy_adequate  lev strat  \\\n",
      "qwen naive planning                 1.00               0.97       0.09   \n",
      "qwen response e2e                   0.97               0.98       0.12   \n",
      "llama naive planning                0.95               0.95       0.12   \n",
      "llama response e2e                  0.95               0.95       0.11   \n",
      "mixtral naive planning              0.95               0.92       0.08   \n",
      "mixtral response e2e                0.93               0.92       0.10   \n",
      "ground truth                        0.98               0.90       0.11   \n",
      "\n",
      "                        jacc strat  role_consistency  \n",
      "qwen naive planning           0.87               1.0  \n",
      "qwen response e2e             0.85               1.0  \n",
      "llama naive planning          0.84               1.0  \n",
      "llama response e2e            0.84               1.0  \n",
      "mixtral naive planning        0.88               1.0  \n",
      "mixtral response e2e          0.85               1.0  \n",
      "ground truth                  0.84               1.0  \n"
     ]
    }
   ],
   "source": [
    "panda_df = pd.DataFrame(data = mtx,  \n",
    "                        index = models,  \n",
    "                        columns = cols) \n",
    "print(panda_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_scores = scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n"
     ]
    }
   ],
   "source": [
    "#SAVE IN data/ FILE\n",
    "file_generated = \"all_criteria\"\n",
    "print('Saving results...')\n",
    "with open(\"results/\"+file_generated+\".txt\", \"w\") as f:\n",
    "  print(panda_df, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Aggregated Scores</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_of_scores = {'id':dic_of_ratings['id'], 'model':dic_of_ratings['model'], 'response':dic_of_ratings['response'], 'total':[], 'logical':[], 'emotional':[], 'social':[]}\n",
    "\n",
    "for i in range(len(dic_of_ratings['id'])):\n",
    "    logical = (dic_of_ratings['usefulness'][i]/3 + dic_of_ratings['fluency'][i]/3  + dic_of_ratings['naturalness'][i] /3) / 3\n",
    "    emo = dic_of_ratings['emotion_adequate'][i] /3\n",
    "    social = (dic_of_ratings['strategy_adequate'][i]/3 + dic_of_ratings['role_consistency'][i] ) / 2\n",
    "    total = (logical + emo + social) / 3\n",
    "\n",
    "    dic_of_scores['total'].append(total)\n",
    "    dic_of_scores['logical'].append(logical)\n",
    "    dic_of_scores['emotional'].append(emo)\n",
    "    dic_of_scores['social'].append(social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE GLOBAL TOTAL SCORE:  0.93\n",
      "\n",
      "AVERAGE GLOBAL LOGICAL SCORE:  0.85\n",
      "AVERAGE GLOBAL EMO SCORE:  0.96\n",
      "AVERAGE GLOBAL SOCIAL SCORE:  0.97\n"
     ]
    }
   ],
   "source": [
    "total = round(mean(dic_of_scores['total']),2)\n",
    "logical = round(mean(dic_of_scores['logical']),2)\n",
    "emo = round(mean(dic_of_scores['emotional']),2)\n",
    "social = round(mean(dic_of_scores['social']),2)\n",
    "\n",
    "\n",
    "print('AVERAGE GLOBAL TOTAL SCORE: ', total)\n",
    "print('')\n",
    "print('AVERAGE GLOBAL LOGICAL SCORE: ', logical)    \n",
    "print('AVERAGE GLOBAL EMO SCORE: ', emo)    \n",
    "print('AVERAGE GLOBAL SOCIAL SCORE: ', social)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_of_models = {m:{'total':[], 'logical':[], 'emotional':[], 'social':[]} for m in models}\n",
    "\n",
    "for i,m in enumerate(dic_of_scores['model']):\n",
    "    mod = dic_of_models[m]\n",
    "    for key, val in mod.items():\n",
    "        mod[key].append(dic_of_scores[key][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total logical 0.8\n",
      "total emotional 0.8\n",
      "total social 0.01\n",
      "logical emotional 0.35\n",
      "logical social -0.5\n",
      "emotional social 0.26\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import itertools\n",
    "\n",
    "items = list(itertools.combinations(list(dic_of_models[models[0]].keys()), 2))\n",
    "\n",
    "for x, y in items:\n",
    "    ref_data = [mean(dic_of_models[m][x]) for m in models]\n",
    "\n",
    "    corres_data = [mean(dic_of_models[m][y]) for m in models]\n",
    "    \n",
    "    corr, _ = stats.pearsonr(ref_data, corres_data)\n",
    "    print(x, y, round(corr,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_2 = np.zeros((len(models), len(dic_of_models[models[0]])))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for j, score in enumerate(list(dic_of_models[model].keys())):\n",
    "        mtx_2[i][j] = round(mean(dic_of_models[model][score]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        total consistency score  logical consistency score  \\\n",
      "qwen naive planning                        0.95                       0.87   \n",
      "qwen response e2e                          0.91                       0.77   \n",
      "llama naive planning                       0.93                       0.86   \n",
      "llama response e2e                         0.93                       0.86   \n",
      "mixtral naive planning                     0.92                       0.86   \n",
      "mixtral response e2e                       0.90                       0.82   \n",
      "ground truth                               0.94                       0.89   \n",
      "\n",
      "                        emotional consistency score  social consistency score  \n",
      "qwen naive planning                            1.00                      0.98  \n",
      "qwen response e2e                              0.97                      0.99  \n",
      "llama naive planning                           0.95                      0.97  \n",
      "llama response e2e                             0.95                      0.97  \n",
      "mixtral naive planning                         0.95                      0.96  \n",
      "mixtral response e2e                           0.93                      0.96  \n",
      "ground truth                                   0.98                      0.95  \n"
     ]
    }
   ],
   "source": [
    "panda_df = pd.DataFrame(data = mtx_2,  \n",
    "                        index = models,  \n",
    "                        columns = [x +' consistency score' for x in list(dic_of_models[model].keys())]) \n",
    "print(panda_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    for k, v in dic_of_models[model].items():\n",
    "        human_scores[model][k] = round(mean(v),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usefulness': 0.87,\n",
       " 'fluency': 0.92,\n",
       " 'naturalness': 0.88,\n",
       " 'emo f1': 0.68,\n",
       " 'emotion_adequate': 0.98,\n",
       " 'strategy_adequate': 0.9,\n",
       " 'lev strat': 0.11,\n",
       " 'jacc strat': 0.84,\n",
       " 'role_consistency': 1,\n",
       " 'total': 0.94,\n",
       " 'logical': 0.89,\n",
       " 'emotional': 0.98,\n",
       " 'social': 0.95}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_scores['ground truth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n"
     ]
    }
   ],
   "source": [
    "#SAVE IN TXT FILE\n",
    "file_generated = \"all_scores\"\n",
    "print('Saving results...')\n",
    "with open(\"results/\"+file_generated+\".txt\", \"w\") as f:\n",
    "  print(panda_df, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Automatic Evaluation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qwen naive planning',\n",
       " 'qwen response e2e',\n",
       " 'qwen 2 step planning',\n",
       " 'qwen 3 steps planning',\n",
       " 'llama naive planning',\n",
       " 'llama response e2e',\n",
       " 'llama 2 step planning',\n",
       " 'llama 3 steps planning',\n",
       " 'mixtral naive planning',\n",
       " 'mixtral response e2e',\n",
       " 'mixtral 2 step planning',\n",
       " 'mixtral 3 steps planning',\n",
       " 'qwen dpo response e2e',\n",
       " 'qwen dpo naive planning',\n",
       " 'ground truth']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [x for x in data_ref.columns if x not in ['context', 'emotions', 'strats', 'gt segments', 'gt segment strats']]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_model = {m: list(data_ref[m]) for m in models}\n",
    "df_per_model['context'] = list(data_ref['context'])\n",
    "df_per_model['emotions'] = [ast.literal_eval(x) for x in list(data_ref['emotions'])]\n",
    "df_per_model['strats'] = list(data_ref['strats'])\n",
    "\n",
    "df_per_model['gt segments'] = [ast.literal_eval(x) for x in list(data_ref['gt segments'])]\n",
    "df_per_model['gt segment strats'] = [ast.literal_eval(x) for x in list(data_ref['gt segment strats'])]\n",
    "\n",
    "df_per_model['ground truth'] = [x[0].upper() + x[1:] for x in df_per_model['ground truth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_per_model['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Comparison-Based Metrics</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lorrainevanel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/lorrainevanel/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/lorrainevanel/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#LOAD METRICS\n",
    "import evaluate\n",
    "\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['bleu', 'rouge', 'bertscore', \"chrf\", 'bertscore_context', 'perplexity', 'meteor']\n",
    "\n",
    "df_results = {model: {metric: [] for metric in metrics} for model in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERST SCORE CONTEXT - RESPONSE\n",
    "contexts = [x.replace(\"Customer(C): \", \"\").replace(\"Agent(A): \", \"\")for x in df_per_model['context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen naive planning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:08<00:00,  4.87s/it]\n",
      "  7%|▋         | 1/15 [02:14<31:25, 134.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen response e2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:22<00:00,  5.90s/it]\n",
      " 13%|█▎        | 2/15 [04:21<28:14, 130.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen 2 step planning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [03:23<00:00, 14.56s/it]\n",
      " 20%|██        | 3/15 [08:48<38:31, 192.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen 3 steps planning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [02:58<00:00, 12.77s/it]\n",
      " 27%|██▋       | 4/15 [12:34<37:43, 205.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama naive planning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:27<00:00,  1.93s/it]\n",
      " 33%|███▎      | 5/15 [13:44<26:07, 156.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama response e2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:13<00:00,  5.27s/it]\n",
      " 40%|████      | 6/15 [15:43<21:35, 143.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama 2 step planning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:57<00:00,  4.11s/it]\n",
      " 47%|████▋     | 7/15 [17:28<17:29, 131.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama 3 steps planning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:36<00:00,  2.59s/it]\n",
      " 53%|█████▎    | 8/15 [18:43<13:12, 113.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixtral naive planning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:12<00:00,  5.16s/it]\n",
      " 60%|██████    | 9/15 [20:39<11:24, 114.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixtral response e2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [03:09<00:00, 13.56s/it]\n",
      " 67%|██████▋   | 10/15 [24:36<12:39, 151.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixtral 2 step planning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [05:44<00:00, 24.58s/it]\n",
      " 73%|███████▎  | 11/15 [31:06<15:00, 225.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixtral 3 steps planning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [03:32<00:00, 15.17s/it]\n",
      " 80%|████████  | 12/15 [35:51<12:10, 243.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen dpo response e2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [09:30<00:00, 40.73s/it]\n",
      " 87%|████████▋ | 13/15 [46:11<11:54, 357.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen dpo naive planning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [08:47<00:00, 37.67s/it]\n",
      " 93%|█████████▎| 14/15 [55:48<07:03, 423.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [03:23<00:00, 14.53s/it]\n",
      "100%|██████████| 15/15 [59:51<00:00, 239.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# CLASSIC NLP REFERENCE COMPARISON AUTOMATED METRICS\n",
    "\n",
    "for model in tqdm(models):\n",
    "    print(model)\n",
    "    if model != \"ground truth\":\n",
    "        #BLEU\n",
    "        bleu= sacrebleu.compute(predictions=df_per_model[model], references=df_per_model['ground truth'])\n",
    "        df_results[model]['bleu'] = round(bleu['score'],2)\n",
    "        \n",
    "        #ROUGE\n",
    "        rouge1 = rouge.compute(predictions=df_per_model[model], references=df_per_model['ground truth'])\n",
    "        rouge1 = rouge1['rouge1']\n",
    "        df_results[model]['rouge'] = round(rouge1,2)\n",
    "        \n",
    "        #BERT SCORE\n",
    "        bs = bertscore.compute(predictions=df_per_model[model], references=df_per_model['ground truth'], lang='fr')\n",
    "        f1 = bs['f1']\n",
    "        avg_f1_bert = round(sum(f1) / len(f1),2)\n",
    "        df_results[model]['bertscore'] = avg_f1_bert\n",
    "        \n",
    "        #CHRF\n",
    "        df_results[model]['chrf'] = round(chrf.compute(predictions=df_per_model[model], references=df_per_model['ground truth'])['score'],2)\n",
    "\n",
    "        #METEOR\n",
    "        df_results[model]['meteor'] = round(meteor.compute(predictions=df_per_model[model], references=df_per_model['ground truth'])['meteor'], 2)\n",
    "    \n",
    "    #BERTSCRE RESPONSE / CONTEXT\n",
    "    bs_c = bertscore.compute(predictions=df_per_model[model], references=contexts, lang='fr')\n",
    "    f1_c = bs_c['f1']\n",
    "    avg_f1_bert_c = round(sum(f1_c) / len(f1_c),2)\n",
    "    df_results[model]['bertscore_context'] = f1_c\n",
    "\n",
    "    #PERPLEXITY\n",
    "    ppl = perplexity.compute(model_id='gpt2', predictions=df_per_model[model])\n",
    "    df_results[model]['perplexity'] = ppl['perplexities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESPONSE MEAN LENGTH\n",
    "for model in models:\n",
    "    df_results[model]['avg length char'] = [len(str(x)) for x in df_per_model[model]]\n",
    "    df_results[model]['avg length words'] = [len(str(x).split(' ')) for x in df_per_model[model]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments 2 & 3/full_evaluation_pipeline.ipynb Cell 60\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39myanzhu_metrics\u001b[39;00m \u001b[39mimport\u001b[39;00m lexical_diversity, semantic_diversity\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y115sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y115sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df_results[model][\u001b[39m'\u001b[39m\u001b[39msemantic diversity\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(semantic_diversity([x \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m df_per_model[model] \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(x, \u001b[39mstr\u001b[39;49m)]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y115sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df_results[model][\u001b[39m'\u001b[39m\u001b[39mlexical diversity\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m lexical_diversity([x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m df_per_model[model] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mstr\u001b[39m)])\n",
      "File \u001b[0;32m~/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments 2 & 3/yanzhu_metrics.py:108\u001b[0m, in \u001b[0;36msemantic_diversity\u001b[0;34m(lines, language)\u001b[0m\n\u001b[1;32m    101\u001b[0m     sentences \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sents\n\u001b[1;32m    103\u001b[0m \u001b[39m# Randomly sample sentences\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m#random.seed(42)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m#sentences = random.sample(sentences, n)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[39m# Load the sentence transformer model\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39;49m\u001b[39msentence-transformers/all-mpnet-base-v2\u001b[39;49m\u001b[39m'\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda:1\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    110\u001b[0m \u001b[39m# Encode sentences into embeddings\u001b[39;00m\n\u001b[1;32m    111\u001b[0m sentence_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(sentences, batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, show_progress_bar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, normalize_embeddings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:348\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_hpu_graph_enabled \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_prompt_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_prompt_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[39m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[39m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m   1160\u001b[0m         device,\n\u001b[1;32m   1161\u001b[0m         dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1162\u001b[0m         non_blocking,\n\u001b[1;32m   1163\u001b[0m     )\n\u001b[1;32m   1164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot copy out of meta tensor; no data!\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from yanzhu_metrics import lexical_diversity, semantic_diversity\n",
    "\n",
    "for model in models:\n",
    "    df_results[model]['semantic diversity'] = float(semantic_diversity([x for x in df_per_model[model] if isinstance(x, str)]))\n",
    "    df_results[model]['lexical diversity'] = lexical_diversity([x for x in df_per_model[model] if isinstance(x, str)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[models[0]].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Socio-Emotional Criteria</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_punct(text):\n",
    "\n",
    "    punc = ['.', '!', '?']\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        for p in punc:\n",
    "            text = text.replace(p, p+'<SEP>')\n",
    "        segs = text.split('<SEP>')\n",
    "\n",
    "        temp = []\n",
    "        for seg in segs:\n",
    "            if len(seg.strip()) > 0:\n",
    "                temp.append(seg.strip())\n",
    "        \n",
    "        return temp\n",
    "    \n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_segments = {m:{'response': df_per_model[m], 'full_annots': [], 'segments': [split_punct(x) for x in df_per_model[m]], 'annots':[], 'emotion': []} for m in models if m != 'ground truth'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_emotions = []\n",
    "\n",
    "for x in df_per_model['emotions']:\n",
    "    if x == []:\n",
    "        ref_emotions.append('neutre')\n",
    "    \n",
    "    else:\n",
    "        ref_emotions.append(x[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = {'ground truth': df_per_model['ground truth'], 'full_annots': df_per_model['strats'], 'segments': df_per_model['gt segments'], 'annots': df_per_model['gt segment strats'], 'emotion': ref_emotions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Emotional Score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213/213 [00:14<00:00, 14.56it/s]\n",
      "100%|██████████| 213/213 [00:13<00:00, 15.52it/s]\n",
      "100%|██████████| 213/213 [00:14<00:00, 14.82it/s]\n",
      "100%|██████████| 213/213 [00:13<00:00, 15.57it/s]\n",
      "100%|██████████| 213/213 [00:10<00:00, 20.60it/s]\n",
      "100%|██████████| 213/213 [00:10<00:00, 20.12it/s]\n",
      "100%|██████████| 213/213 [00:10<00:00, 19.75it/s]\n",
      "100%|██████████| 213/213 [00:10<00:00, 19.73it/s]\n",
      "100%|██████████| 213/213 [00:12<00:00, 17.20it/s]\n",
      "100%|██████████| 213/213 [00:12<00:00, 16.44it/s]\n",
      "100%|██████████| 213/213 [00:14<00:00, 14.98it/s]\n",
      "100%|██████████| 213/213 [00:13<00:00, 15.65it/s]\n",
      "100%|██████████| 213/213 [00:18<00:00, 11.78it/s]\n",
      "100%|██████████| 213/213 [00:13<00:00, 16.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#Presence of Emotion = Classifier\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"classifier/emo_class/checkpoint-40\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "config = model.config\n",
    "\n",
    "\n",
    "for m in dic_segments.keys():\n",
    "\n",
    "    texts = dic_segments[m]['response']\n",
    "    preds = []\n",
    "\n",
    "    for text in tqdm(texts):\n",
    "        if isinstance(text, str):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512) \n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            prediction = torch.nn.functional.softmax(outputs.logits, dim=-1) \n",
    "            predicted_emotion = prediction.argmax().item()\n",
    "            preds.append(config.id2label[predicted_emotion])\n",
    "        else:\n",
    "            preds.append('neutre')\n",
    "\n",
    "    dic_segments[m]['emotion'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen naive planning : Accuracy  0.66  - Precision  0.73  - Recall :  0.66  - F1 :  0.69\n",
      "qwen response e2e : Accuracy  0.64  - Precision  0.67  - Recall :  0.64  - F1 :  0.65\n",
      "qwen 2 step planning : Accuracy  0.58  - Precision  0.68  - Recall :  0.58  - F1 :  0.62\n",
      "qwen 3 steps planning : Accuracy  0.56  - Precision  0.67  - Recall :  0.56  - F1 :  0.6\n",
      "llama naive planning : Accuracy  0.67  - Precision  0.69  - Recall :  0.67  - F1 :  0.68\n",
      "llama response e2e : Accuracy  0.61  - Precision  0.69  - Recall :  0.61  - F1 :  0.64\n",
      "llama 2 step planning : Accuracy  0.58  - Precision  0.68  - Recall :  0.58  - F1 :  0.62\n",
      "llama 3 steps planning : Accuracy  0.59  - Precision  0.69  - Recall :  0.59  - F1 :  0.63\n",
      "mixtral naive planning : Accuracy  0.46  - Precision  0.69  - Recall :  0.46  - F1 :  0.53\n",
      "mixtral response e2e : Accuracy  0.5  - Precision  0.67  - Recall :  0.5  - F1 :  0.55\n",
      "mixtral 2 step planning : Accuracy  0.58  - Precision  0.7  - Recall :  0.58  - F1 :  0.63\n",
      "mixtral 3 steps planning : Accuracy  0.61  - Precision  0.7  - Recall :  0.61  - F1 :  0.65\n",
      "qwen dpo response e2e : Accuracy  0.42  - Precision  0.65  - Recall :  0.42  - F1 :  0.49\n",
      "qwen dpo naive planning : Accuracy  0.36  - Precision  0.68  - Recall :  0.36  - F1 :  0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorrainevanel/anaconda3/envs/lo/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lorrainevanel/anaconda3/envs/lo/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lorrainevanel/anaconda3/envs/lo/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lorrainevanel/anaconda3/envs/lo/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lorrainevanel/anaconda3/envs/lo/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lorrainevanel/anaconda3/envs/lo/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lorrainevanel/anaconda3/envs/lo/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/lorrainevanel/anaconda3/envs/lo/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_recall_fscore_support\n",
    "\n",
    "for m in dic_segments.keys():\n",
    "    scores = precision_recall_fscore_support(ref['emotion'], dic_segments[m]['emotion'], average='weighted')\n",
    "\n",
    "    print(m, ': Accuracy ', round(accuracy_score(ref['emotion'], dic_segments[m]['emotion']),2), ' - Precision ', round(scores[0], 2), ' - Recall : ', round(scores[1], 2), ' - F1 : ', round(scores[2], 2))\n",
    "\n",
    "    df_results[m]['emotion f1'] = round(f1_score(ref['emotion'], dic_segments[m]['emotion'], average='weighted'),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Social Score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while deserializing header: MetadataIncompleteBuffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments 2 & 3/full_evaluation_pipeline.ipynb Cell 71\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y132sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclassifier/camembert-large/checkpoint-770\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y132sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name) \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y132sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y132sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y132sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m dic_segments\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/site-packages/transformers/modeling_utils.py:3964\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3944\u001b[0m     resolved_archive_file, sharded_metadata \u001b[39m=\u001b[39m get_checkpoint_shard_files(\n\u001b[1;32m   3945\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3946\u001b[0m         resolved_archive_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3956\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[1;32m   3957\u001b[0m     )\n\u001b[1;32m   3959\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3960\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3961\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(resolved_archive_file, \u001b[39mstr\u001b[39m)\n\u001b[1;32m   3962\u001b[0m     \u001b[39mand\u001b[39;00m resolved_archive_file\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.safetensors\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3963\u001b[0m ):\n\u001b[0;32m-> 3964\u001b[0m     \u001b[39mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m   3965\u001b[0m         metadata \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mmetadata()\n\u001b[1;32m   3967\u001b[0m     \u001b[39mif\u001b[39;00m metadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3968\u001b[0m         \u001b[39m# Assume it's a pytorch checkpoint (introduced for timm checkpoints)\u001b[39;00m\n",
      "\u001b[0;31mSafetensorError\u001b[0m: Error while deserializing header: MetadataIncompleteBuffer"
     ]
    }
   ],
   "source": [
    "# train model to classify dialogue strategies, compare sequences in predicted responses to ground truth, use levenshtein to compute a score.\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"classifier/camembert-large/checkpoint-770\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "config = model.config\n",
    "\n",
    "for m in dic_segments.keys():\n",
    "\n",
    "    texts = dic_segments[m]['segments']\n",
    "    annots = []\n",
    "\n",
    "    for segments in tqdm(texts):\n",
    "        temp = []\n",
    "        if isinstance(segments, list):\n",
    "            for segment in segments:\n",
    "                if isinstance(segment.strip(), str):\n",
    "\n",
    "                    inputs = tokenizer(segment.strip(), return_tensors=\"pt\", padding=True, truncation=True, max_length=512) \n",
    "                    outputs = model(**inputs)\n",
    "\n",
    "                    prediction = torch.nn.functional.softmax(outputs.logits, dim=-1) \n",
    "                    predicted_emotion = prediction.argmax().item()\n",
    "                    temp.append(config.id2label[predicted_emotion])\n",
    "\n",
    "        annots.append(temp)\n",
    "\n",
    "    dic_segments[m]['annots'] = annots\n",
    "    dic_segments[m]['full_annots'] = ['+'.join(x) for x in annots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "StatisticsError",
     "evalue": "mean requires at least one data point",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments 2 & 3/full_evaluation_pipeline.ipynb Cell 72\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y133sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdistance\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y133sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m dic_segments\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y133sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     lev \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(mean([levenshtein\u001b[39m.\u001b[39;49mnormalized_similarity(ref[\u001b[39m'\u001b[39;49m\u001b[39mannots\u001b[39;49m\u001b[39m'\u001b[39;49m][i], dic_segments[m][\u001b[39m'\u001b[39;49m\u001b[39mannots\u001b[39;49m\u001b[39m'\u001b[39;49m][i]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(dic_segments[m][\u001b[39m'\u001b[39;49m\u001b[39mannots\u001b[39;49m\u001b[39m'\u001b[39;49m]))]),\u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y133sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     jacc \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(mean([distance\u001b[39m.\u001b[39mjaccard(\u001b[39mset\u001b[39m(ref[\u001b[39m'\u001b[39m\u001b[39mannots\u001b[39m\u001b[39m'\u001b[39m][i]), \u001b[39mset\u001b[39m(dic_segments[m][\u001b[39m'\u001b[39m\u001b[39mannots\u001b[39m\u001b[39m'\u001b[39m][i])) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dic_segments[m][\u001b[39m'\u001b[39m\u001b[39mannots\u001b[39m\u001b[39m'\u001b[39m]))]), \u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorrainevanel/Documents/GitHub/Natural-Language-Generation-for-Socially-Competent-Agents/Experiments%202%20%26%203/full_evaluation_pipeline.ipynb#Y133sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(m, \u001b[39m'\u001b[39m\u001b[39m: Levenshtein Similarity \u001b[39m\u001b[39m'\u001b[39m, lev, \u001b[39m'\u001b[39m\u001b[39m - Jaccard Similarity \u001b[39m\u001b[39m'\u001b[39m, jacc)\n",
      "File \u001b[0;32m~/anaconda3/envs/lo/lib/python3.9/statistics.py:315\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    313\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m    314\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mraise\u001b[39;00m StatisticsError(\u001b[39m'\u001b[39m\u001b[39mmean requires at least one data point\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    316\u001b[0m T, total, count \u001b[39m=\u001b[39m _sum(data)\n\u001b[1;32m    317\u001b[0m \u001b[39massert\u001b[39;00m count \u001b[39m==\u001b[39m n\n",
      "\u001b[0;31mStatisticsError\u001b[0m: mean requires at least one data point"
     ]
    }
   ],
   "source": [
    "from textdistance import levenshtein\n",
    "import distance\n",
    "\n",
    "for m in dic_segments.keys():\n",
    "    lev = round(mean([levenshtein.normalized_similarity(ref['annots'][i], dic_segments[m]['annots'][i]) for i in range(len(dic_segments[m]['annots']))]),2)\n",
    "    jacc = round(mean([distance.jaccard(set(ref['annots'][i]), set(dic_segments[m]['annots'][i])) for i in range(len(dic_segments[m]['annots']))]), 2)\n",
    "\n",
    "    print(m, ': Levenshtein Similarity ', lev, ' - Jaccard Similarity ', jacc)\n",
    "    df_results[m]['Levenshtein Similarity DS'] = lev\n",
    "    df_results[m]['Jaccard Similarity DS'] = jacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Saving Results!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "res_arry = np.zeros((len(models), len(df_results[models[0]].keys())))\n",
    "scores_all = {m:{k:0 for k in df_results[models[0]].keys()} for m in models}\n",
    "\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for j, metric in enumerate(df_results[model].keys()):\n",
    "\n",
    "        v = df_results[model][metric]\n",
    "\n",
    "        if v != []:        \n",
    "            if isinstance(v, list):\n",
    "                if metric == \"score\":\n",
    "                    res_arry[i][j] = 1 - round(mean(v), 2)\n",
    "                    scores_all[model][metric] =1 - round(mean(v), 2)\n",
    "\n",
    "                else:\n",
    "                    res_arry[i][j] = round(mean(v), 2)\n",
    "                    scores_all[model][metric] = round(mean(v), 2)\n",
    "                \n",
    "            if isinstance(v, float):\n",
    "                res_arry[i][j] = round(v,2)\n",
    "                scores_all[model][metric] = round(v,2)\n",
    "\n",
    "            elif isinstance(v, int):\n",
    "                res_arry[i][j] = v\n",
    "                scores_all[model][metric] = v\n",
    "\n",
    "        elif metric == \"semantic similarity\":\n",
    "            res_arry[i][j] = round(v,2)\n",
    "            scores_all[model][metric] = res_arry[i][j]\n",
    "\n",
    "        elif metric == \"rouge\" and model != \"ground truth\":\n",
    "            res_arry[i][j] = v\n",
    "            scores_all[model][metric] = v\n",
    "\n",
    "        elif metric == \"meteor\" and model != \"ground truth\":\n",
    "            res_arry[i][j] = v\n",
    "            scores_all[model][metric] = v\n",
    "\n",
    "        else:\n",
    "            res_arry[i][j] = None\n",
    "            scores_all[model][metric] = res_arry[i][j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panda_df = pd.DataFrame(data = res_arry,  \n",
    "                        index = models,  \n",
    "                        columns = df_results[models[0]].keys()) \n",
    "print(panda_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE IN TXT FILE\n",
    "file_generated = \"all_results\"\n",
    "print('Saving results...')\n",
    "with open(\"results/\"+file_generated+\".txt\", \"w\") as f:\n",
    "  print(panda_df, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pearson Correlation between Human Evaluation and Automatic Metrics</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_human_annots = human_scores\n",
    "\n",
    "for k, v in all_human_annots.items():\n",
    "    for a, b in filter_scores[k].items():\n",
    "        all_human_annots[k][a] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondance = {'filtered weighted': 'bertscore_context', 'fluency': ['perplexity'], \"naturalness\": 'semantic diversity', \"emotion_adequate\": 'emotion f1', 'strategy_adequate': 'Levenshtein Similarity DS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "correl_scores= {k:0 for k in list(correspondance.keys())}\n",
    "\n",
    "for ref, corres in correspondance.items():\n",
    "    if isinstance(corres, list):\n",
    "        temp = []\n",
    "        for metric in corres:\n",
    "            ref_data = [human_scores[m][ref] for m in list(human_scores.keys())if m != \"ground truth\"]\n",
    "            #print(ref_data)\n",
    "\n",
    "            corres_data = [scores_all[m][metric] for m in list(human_scores.keys()) if m != \"ground truth\"]\n",
    "            #print(corres_data)\n",
    "\n",
    "            corr, _ = stats.pearsonr(ref_data, corres_data)\n",
    "            temp.append(corr)\n",
    "        correl_scores[ref] = round(mean(temp),2)\n",
    "    else:\n",
    "        ref_data = [human_scores[m][ref] for m in list(human_scores.keys())if m != \"ground truth\"]\n",
    "        #print(ref_data)\n",
    "\n",
    "        corres_data = [scores_all[m][corres] for m in list(human_scores.keys())if m != \"ground truth\"]\n",
    "        #print(corres_data)        \n",
    "        \n",
    "        corr, _ = stats.pearsonr(ref_data, corres_data)\n",
    "        correl_scores[ref] = round(corr,2)\n",
    "\n",
    "for k, v in correl_scores.items():\n",
    "    print(k, float(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Strategy Specific Metrics</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE STRATEGIES\n",
    "strategies_dic = {'HS': 'Hors-sujet',\n",
    "       \"A\": 'Accord',\n",
    "       \"AGR\": 'Agressivité',\n",
    "       \"BC\": \"Back-channeling\",\n",
    "       \"C\": 'Correction',\n",
    "       \"D\": \"Désaccord\",\n",
    "       \"E\": \"Encouragement\", \n",
    "       \"I\": \"Information\",\n",
    "       \"P\": \"Politesse\",\n",
    "       \"PS\": \"Propositiondesuggestion\",\n",
    "       \"Q\": \"Question\",\n",
    "       \"R\": \"Reformulation\",\n",
    "       \"S\": \"Sympathie\",\n",
    "       \"SD\": \"Self-disclosure\",\n",
    "       \"U\": \"Autre\"}\n",
    "\n",
    "strategies = list(strategies_dic.keys())\n",
    "strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each class use yanzhu's metrics  - Generated AND Reference\n",
    "from yanzhu_metrics import lexical_diversity, semantic_diversity\n",
    "\n",
    "yanzhu_strats = {m: {k:{'semantic':0, 'lexical':0, 'len': 0} for k in strategies} for m in models}\n",
    "\n",
    "for m in models:\n",
    "    print(m)\n",
    "\n",
    "\n",
    "    for strat in tqdm(strategies):\n",
    "\n",
    "        sentences_strat = []\n",
    "        if m == \"ground truth\":\n",
    "             a, b = ref['segments'], ref['annots']\n",
    "\n",
    "        else:\n",
    "             a, b = dic_segments[m]['segments'], dic_segments[m]['annots']\n",
    "\n",
    "        for segment, annot in zip(a,b):\n",
    "            if strat in annot:\n",
    "                sentences_temp = [x for j,x in enumerate(segment) if annot[j] == strat]\n",
    "                for s in sentences_temp:\n",
    "                        if len(s) > 1:\n",
    "                            sentences_strat.append(s)\n",
    "\n",
    "        if len(sentences_strat) > 0:\n",
    "            yanzhu_strats[m][strat]['len'] = len(sentences_strat)\n",
    "            yanzhu_strats[m][strat]['semantic'] = lexical_diversity(sentences_strat)\n",
    "            yanzhu_strats[m][strat]['lexical'] = semantic_diversity(sentences_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_strat= np.zeros((len(models), len(strategies)))\n",
    "for k, v in yanzhu_strats.items():\n",
    "    for strat, value in v.items():\n",
    "        print(k,'\\t', strat, '\\t', value['len'], '\\t', round(value['semantic'],2), '\\t', round(value['lexical'],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_strat= np.zeros((len(models)*2, len(strategies)))\n",
    "\n",
    "i =0\n",
    "for model in models:\n",
    "    for j, strat in enumerate(strategies):\n",
    "        mtx_strat[i][j] = int(yanzhu_strats[model][strat]['semantic'])\n",
    "        mtx_strat[i+1][j] = int(yanzhu_strats[model][strat]['len'])\n",
    "\n",
    "    i+= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_lex= np.zeros((len(models)*2, len(strategies)))\n",
    "\n",
    "i =0\n",
    "for model in models:\n",
    "    for j, strat in enumerate(strategies):\n",
    "        mtx_lex[i][j] = round(yanzhu_strats[model][strat]['lexical'],2)\n",
    "        mtx_lex[i+1][j] = int(yanzhu_strats[model][strat]['len'])\n",
    "\n",
    "    i+= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "\n",
    "for m in models:\n",
    "    cols.append(m)\n",
    "    cols.append(m + ' len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panda_df = pd.DataFrame(mtx_strat,  \n",
    "                        index = cols,  \n",
    "                        columns = strategies) \n",
    "print(panda_df)\n",
    "\n",
    "#SAVE IN TXT FILE\n",
    "file_generated = \"yanzhu_sem\"\n",
    "print('Saving results...')\n",
    "with open(\"results/\"+file_generated+\".txt\", \"w\") as f:\n",
    "  print(panda_df, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panda_df = pd.DataFrame(mtx_lex,  \n",
    "                        index = cols,  \n",
    "                        columns = strategies) \n",
    "print(panda_df)\n",
    "\n",
    "#SAVE IN TXT FILE\n",
    "file_generated = \"yanzhu_lex\"\n",
    "print('Saving results...')\n",
    "with open(\"results/\"+file_generated+\".txt\", \"w\") as f:\n",
    "  print(panda_df, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each strat, get avg for context & segment - Generated AND Reference \n",
    "#especially for reformulation\n",
    "bert_strats = {m:{k:{'len': 0, 'bertscore': 0}  for k in strategies} for m in models}\n",
    "\n",
    "\n",
    "for m in models:\n",
    "    print(m)\n",
    "\n",
    "    for strat in tqdm(strategies):\n",
    "\n",
    "        sentences_strat = []\n",
    "        if m == \"ground truth\":\n",
    "             a, b = ref['segments'], ref['annots']\n",
    "\n",
    "        else:\n",
    "             a, b = dic_segments[m]['segments'], dic_segments[m]['annots']\n",
    "\n",
    "        sentences, context_sentences = [], [] \n",
    "\n",
    "        i = 0\n",
    "        for segment, annot in zip(a,b):\n",
    "            if strat in annot:\n",
    "                sentences_temp = [x for j,x in enumerate(segment) if annot[j] == strat]\n",
    "                for s in sentences_temp:\n",
    "                    sentences.append(s)\n",
    "                    context_sentences.append(df_per_model['context'][i])\n",
    "            i +=1\n",
    "        \n",
    "        if len(sentences) > 0:\n",
    "            bert_score_strat = bertscore.compute(predictions=sentences, references=context_sentences, lang='fr')\n",
    "            f1_strat = bert_score_strat['f1']\n",
    "            bert_strats[m][strat]['len'] = len(sentences)\n",
    "            bert_strats[m][strat]['bertscore'] = sum(f1_strat) / len(f1_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx_bert= np.zeros((len(models)*2, len(strategies)))\n",
    "\n",
    "i =0\n",
    "for model in models:\n",
    "    for j, strat in enumerate(strategies):\n",
    "        if bert_strats[model][strat]['bertscore'] > 0:\n",
    "            mtx_bert[i][j] = round(bert_strats[model][strat]['bertscore'],2)\n",
    "            mtx_bert[i+1][j] = int(bert_strats[model][strat]['len'])\n",
    "\n",
    "    i+= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panda_df = pd.DataFrame(mtx_bert,  \n",
    "                        index = cols,  \n",
    "                        columns = strategies) \n",
    "print(panda_df)\n",
    "\n",
    "#SAVE IN TXT FILE\n",
    "file_generated = \"bert_strat\"\n",
    "print('Saving results...')\n",
    "with open(\"results/\"+file_generated+\".txt\", \"w\") as f:\n",
    "  print(panda_df, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in bert_strats.items():\n",
    "    for strat, value in v.items():\n",
    "        if value['len'] >0:\n",
    "            print(k,' - ', strat, ' - len: ', value['len'], ' bertscore : ', value['bertscore'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
